{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid_search & cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "d1 = spark.read.parquet('/data/user/hive/warehouse/ian/feature/unrecognized/*')\n",
    "d2 = spark.read.parquet('/data/user/hive/warehouse/ian/feature/recognized/*')\n",
    "d2 = d2.sample(0.20,seed=123)\n",
    "\n",
    "df = d1.union(d2)\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler,StandardScaler,VectorAssembler,StringIndexer\n",
    "\n",
    "vec = VectorAssembler(inputCols=['f1','f2','f3','f4','f5','f6','f7'],outputCol='features')\n",
    "vec_ = vec.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = vec_.randomSplit([0.8,0.2],seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassificationModel' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-73b1ba5a79f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcvModel_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mbest_model_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mbest_model_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestClassificationModel' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "import numpy as np\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder()\\\n",
    "       .addGrid(rf.maxDepth,[3,4,5,6,7,8,9,10])\\\n",
    "       .addGrid(rf.numTrees,[10,15,20,25,30])\\\n",
    "       .build()\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                            estimatorParamMaps=paramGrid_rf,\n",
    "                            evaluator=BinaryClassificationEvaluator(),\n",
    "                            numFolds=5)\n",
    "\n",
    "cvModel_rf = crossval_rf.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_rf = cvModel_rf.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9450373479023623"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "my_eval_rf = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "my_eval_rf.evaluate(best_model_rf.transform(testDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----+---+----+----+----+---+---+\n",
      "|val                            |f1  |f2 |f3  |f4  |f5  |f6 |f7 |\n",
      "+-------------------------------+----+---+----+----+----+---+---+\n",
      "|uplus-haier-0601-29a4-v5-sapkz |30.0|0.0|8.0 |17.0|0.0 |5.0|0.0|\n",
      "|BRW0C96E67F9881                |15.0|1.0|9.0 |0.0 |6.0 |0.0|0.0|\n",
      "|05af9271958ff346b26a47b3a5ef7a7|31.0|0.0|20.0|11.0|0.0 |0.0|0.0|\n",
      "|054297ed1620ee2506981fcc2aee7da|31.0|0.0|19.0|12.0|0.0 |0.0|0.0|\n",
      "|ZTE24:58:6e:84:98:38           |20.0|1.0|11.0|1.0 |3.0 |5.0|0.0|\n",
      "|05bfa2a9116d3f3c04535b4cedff9e4|31.0|0.0|17.0|14.0|0.0 |0.0|0.0|\n",
      "|GM-T9+                         |6.0 |1.0|1.0 |0.0 |3.0 |1.0|0.0|\n",
      "|N6MK2                          |5.0 |1.0|2.0 |0.0 |3.0 |0.0|0.0|\n",
      "|E55CA-TC3551A-M                |15.0|1.0|6.0 |0.0 |7.0 |2.0|0.0|\n",
      "|0ae6c6f54f4a69c2acfe3588223eb19|31.0|0.0|18.0|13.0|0.0 |0.0|0.0|\n",
      "|ZTE8c:68:c8:b1:d2:78           |20.0|1.0|8.0 |4.0 |3.0 |5.0|0.0|\n",
      "|ZTE8c:68:c8:bf:94:78           |20.0|1.0|8.0 |4.0 |3.0 |5.0|0.0|\n",
      "|AUTOBVT-IV5PHI2                |15.0|1.0|2.0 |0.0 |12.0|1.0|0.0|\n",
      "|Flower                         |6.0 |1.0|0.0 |5.0 |1.0 |0.0|0.0|\n",
      "|K                              |1.0 |1.0|0.0 |0.0 |1.0 |0.0|0.0|\n",
      "|6XEQCJF4RUHSPMY                |15.0|0.0|2.0 |0.0 |13.0|0.0|0.0|\n",
      "|lpBlueprint                    |11.0|0.0|0.0 |10.0|1.0 |0.0|0.0|\n",
      "|PFLOXT6EUTMU5CM                |15.0|1.0|2.0 |0.0 |13.0|0.0|0.0|\n",
      "|0818d94e84f9425a310a730aeff7dde|31.0|0.0|19.0|12.0|0.0 |0.0|0.0|\n",
      "|0992b25fc821e6500627c81ef551fed|31.0|0.0|21.0|10.0|0.0 |0.0|0.0|\n",
      "+-------------------------------+----+---+----+----+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_visual_unknown/*',sep='\\x01')\n",
    "d1 = d.select('_c2').withColumnRenamed('_c2','val').distinct().dropna()\n",
    "\n",
    "d1 = d1.withColumn('f1',length(col('val')))\n",
    "\n",
    "d1 = d1.withColumn('f2',when(d1.val.startswith('A')|d1.val.startswith('B')|d1.val.startswith('C')\\\n",
    "                             |d1.val.startswith('D')|d1.val.startswith('E')|d1.val.startswith('F')\\\n",
    "                             |d1.val.startswith('G')|d1.val.startswith('H')|d1.val.startswith('I')\\\n",
    "                             |d1.val.startswith('J')|d1.val.startswith('K')|d1.val.startswith('L')\\\n",
    "                             |d1.val.startswith('M')|d1.val.startswith('N')|d1.val.startswith('O')\\\n",
    "                             |d1.val.startswith('P')|d1.val.startswith('Q')|d1.val.startswith('R')\\\n",
    "                             |d1.val.startswith('S')|d1.val.startswith('T')|d1.val.startswith('U')\\\n",
    "                             |d1.val.startswith('V')|d1.val.startswith('W')|d1.val.startswith('X')\\\n",
    "                             |d1.val.startswith('Y')|d1.val.startswith('Z'),1).otherwise(0))\n",
    "\n",
    "import re\n",
    "\n",
    "num_regex = re.compile(r'[0-9]') #数字\n",
    "xiaoxiezimu_regex = re.compile(r'[a-z]')#小写字母\n",
    "daxiezimu_regex = re.compile(r'[A-Z]') #大写字母 \n",
    "#hanzi_regex = re.compile(r'[\\u4E00-\\u9FA5]')#汉字\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "num = udf(lambda x: len(num_regex.findall(x)))\n",
    "xiaoxie = udf(lambda x: len(xiaoxiezimu_regex.findall(x)))\n",
    "daxie = udf(lambda x: len(daxiezimu_regex.findall(x)))\n",
    "\n",
    "d1 = d1.withColumn('f3',num('val'))\n",
    "d1 = d1.withColumn('f4',xiaoxie('val'))\n",
    "d1 = d1.withColumn('f5',daxie('val'))\n",
    "\n",
    "# 特征字符串长度 f1\n",
    "# 首字母是否大写 f2\n",
    "# 数字字符数量   f3\n",
    "# 小写字母数量   f4\n",
    "# 大写字母数量   f5\n",
    "# 特殊符号-_:的数量 f6\n",
    "# 空格的数量 f7\n",
    "\n",
    "\n",
    "# 统计下划线个数\n",
    "def xiahuaxian_count(s):\n",
    "    xiahuaxian_counts=0\n",
    "    for c in s:\n",
    "        xiahuaxian_split_list = c.split('_')\n",
    "        xiahuaxian_counts += len(xiahuaxian_split_list) - 1\n",
    "    return xiahuaxian_counts\n",
    "\n",
    "# 统计中划线个数\n",
    "def zhonghuaxian_count(s):\n",
    "    zhonghuaxian_counts=0\n",
    "    for c in s:\n",
    "        zhonghuaxian_split_list = c.split('-')\n",
    "        zhonghuaxian_counts += len(zhonghuaxian_split_list) - 1\n",
    "    return zhonghuaxian_counts\n",
    "\n",
    "# 统计冒号个数\n",
    "def maohao_count(s):\n",
    "    maohao_counts=0\n",
    "    for c in s:\n",
    "        maohao_split_list = c.split(':')\n",
    "        maohao_counts += len(maohao_split_list) - 1\n",
    "    return maohao_counts\n",
    "\n",
    "def teshu_count(s):\n",
    "    teshu_counts=0\n",
    "    a_counts=0\n",
    "    b_counts=0\n",
    "    c_counts=0\n",
    "    for c in s:\n",
    "        a_split_list = c.split('_')\n",
    "        a_counts += len(a_split_list) - 1\n",
    "        \n",
    "        b_split_list = c.split('-')\n",
    "        b_counts += len(b_split_list) - 1\n",
    "        \n",
    "        c_split_list = c.split(':')\n",
    "        c_counts += len(c_split_list) - 1\n",
    "        \n",
    "        teshu_counts = a_counts + b_counts + c_counts\n",
    "    return teshu_counts\n",
    "        \n",
    "\n",
    "# 统计空格个数\n",
    "def space_count(s):\n",
    "    space_counts=0\n",
    "    for c in s:\n",
    "        space_split_list = c.split(' ')\n",
    "        space_counts += len(space_split_list) - 1\n",
    "    return space_counts\n",
    "\n",
    "teshu = udf(lambda x: teshu_count(x))\n",
    "kongge = udf(lambda x: space_count(x))\n",
    "\n",
    "\n",
    "d1 = d1.withColumn('f6',teshu('val'))\n",
    "d1 = d1.withColumn('f7',kongge('val'))\n",
    "\n",
    "\n",
    "d1 = d1.select('val',col('f1').cast('float'),col('f2').cast('float'),col('f3').cast('float'),col('f4').cast('float'),col('f5').cast('float'),col('f6').cast('float'),col('f7').cast('float'))\n",
    "\n",
    "d1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5804"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('val', 'string'),\n",
       " ('f1', 'float'),\n",
       " ('f2', 'float'),\n",
       " ('f3', 'float'),\n",
       " ('f4', 'float'),\n",
       " ('f5', 'float'),\n",
       " ('f6', 'float'),\n",
       " ('f7', 'float')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler,StandardScaler,VectorAssembler,StringIndexer\n",
    "\n",
    "unknow = VectorAssembler(inputCols=['f1','f2','f3','f4','f5','f6','f7'],outputCol='features')\n",
    "unknow = vec.transform(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cvModel_rf.transform(unknow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('val', 'string'),\n",
       " ('f1', 'float'),\n",
       " ('f2', 'float'),\n",
       " ('f3', 'float'),\n",
       " ('f4', 'float'),\n",
       " ('f5', 'float'),\n",
       " ('f6', 'float'),\n",
       " ('f7', 'float'),\n",
       " ('features', 'vector'),\n",
       " ('rawPrediction', 'vector'),\n",
       " ('probability', 'vector'),\n",
       " ('prediction', 'double')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "unlist = udf(lambda x: float(list(x)[1]), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = t.select('val',unlist('probability').alias('probability'),'prediction').filter(t.prediction == 1).filter(t.val != 'unknown').filter(t.val != 'empty').filter(t.val != 'NONE').filter(t.val != 'none').filter(t.val != 'N/A').filter(t.val != 'normal').filter(t.val != 'anonymous').filter(t.val != 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+----------+\n",
      "|val                    |probability       |prediction|\n",
      "+-----------------------+------------------+----------+\n",
      "|AUTOBVT-IV5PHI2        |0.9152087674233372|1.0       |\n",
      "|DEEP-2019OFISTD        |0.8221985482569627|1.0       |\n",
      "|IDEA-20170821AZ        |0.6022177298000291|1.0       |\n",
      "|gaoxinqu-0074.9c5b.e7fa|0.9989348193771349|1.0       |\n",
      "|DEEP-2019QLTSAS        |0.8221985482569627|1.0       |\n",
      "|AUTOBVT-5J3GA4H        |0.9148265314337913|1.0       |\n",
      "|XZ-201705231143        |0.530341964118253 |1.0       |\n",
      "|AUTOBVT-61AGAV1        |0.9148265314337913|1.0       |\n",
      "|luke-HP-15-Notebook-PC |0.7408531144522307|1.0       |\n",
      "|XCB-20160304VKM        |0.6022177298000291|1.0       |\n",
      "+-----------------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|0.0       |5508 |\n",
      "|1.0       |296  |\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t.select('val',unlist('probability').alias('probability'),'prediction').groupBy('prediction').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_hdfs = pred.repartition(1)\n",
    "\n",
    "#pred_hdfs.write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/prediction/p2',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel_rf.save('hdfs:///data/user/hive/warehouse/ian/model/model4')\n",
    "# m4 = cvModel_rf.load('hdfs:///data/user/hive/warehouse/ian/model/model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# 模型预测\n",
    "prediction = lrModel.transform(testData)\n",
    "\n",
    "# ROC score\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "grid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# Create 10-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=10)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "#evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# 构建模型\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=10, maxBins=30, labelCol=\"label\", seed=123)\n",
    "# 十折交叉验证\n",
    "grid = (ParamGridBuilder().addGrid(rf.numTrees, [1, 3, 5])\n",
    "                          .addGrid(rf.maxDepth, [3, 5, 7, 10])\n",
    "                          .addGrid(rf.maxBins, [20, 30, 40])\n",
    "                          .build())\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=grid,\n",
    "                    numFolds=10)\n",
    "cvModel_rf = cv.fit(trainingData)\n",
    "\n",
    "# 模型预测 ROC\n",
    "predictions = cvModel_rf.transform(testData)\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
