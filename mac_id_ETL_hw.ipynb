{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hw mac_id_ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = spark.read.csv('/user/maxnet/database/sig.db/term_desc_all',sep='\\x01')\n",
    "d1 = d1.withColumnRenamed('_c0','term_id')\\\n",
    "       .withColumnRenamed('_c1','brand')\\\n",
    "       .withColumnRenamed('_c2','cn_name')\\\n",
    "       .withColumnRenamed('_c3','en_name')\\\n",
    "       .withColumnRenamed('_c4','type1')\\\n",
    "       .withColumnRenamed('_c5','type2')\\\n",
    "       .withColumnRenamed('_c6','os')\\\n",
    "       .withColumnRenamed('_c7','dtime')\\\n",
    "       .withColumnRenamed('_c8','price')\\\n",
    "       .withColumnRenamed('_c9','remarks')\\\n",
    "       .withColumnRenamed('_c10','prio')\n",
    "\n",
    "d2 = spark.read.csv('/user/maxnet/database/sig.db/term_oui',sep='\\x01')\n",
    "d2 = d2.withColumnRenamed('_c0','mac_6').withColumnRenamed('_c1','manu')\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_mac_res',sep='\\x01')\n",
    "d = d.withColumnRenamed('_c0','mac').withColumnRenamed('_c1','id')\n",
    "d = d.withColumn('m1_6',substring(d.mac,1,6))\n",
    "\n",
    "tmp_1 = d.join(d2,d.m1_6==d2.mac_6)\n",
    "\n",
    "df = tmp_1.join(d1,tmp_1.id == d1.term_id).select('mac','id','manu','brand','type1','type2','cn_name').distinct().dropna()\n",
    "\n",
    "hw = df.filter(df.brand == '华为')\n",
    "\n",
    "hw = hw.withColumn('m1',substring(hw.mac,1,1))\\\n",
    "       .withColumn('m2',substring(hw.mac,2,1))\\\n",
    "       .withColumn('m3',substring(hw.mac,3,1))\\\n",
    "       .withColumn('m4',substring(hw.mac,4,1))\\\n",
    "       .withColumn('m5',substring(hw.mac,5,1))\\\n",
    "       .withColumn('m6',substring(hw.mac,6,1))\\\n",
    "       .withColumn('m7',substring(hw.mac,7,1))\\\n",
    "       .withColumn('m8',substring(hw.mac,8,1))\\\n",
    "       .withColumn('m9',substring(hw.mac,9,1))\\\n",
    "       .withColumn('m10',substring(hw.mac,10,1))\\\n",
    "       .withColumn('m11',substring(hw.mac,11,1))\\\n",
    "       .withColumn('m12',substring(hw.mac,12,1))  \n",
    "\n",
    "tmp = hw.groupBy('cn_name').count().withColumnRenamed('count','c')\n",
    "use = tmp.filter(tmp.c > 200000)\n",
    "use_pandas = use.toPandas()\n",
    "\n",
    "use_id = list(use_pandas.cn_name)\n",
    "\n",
    "def filt(s):\n",
    "    for x in use_id:\n",
    "        if x == s:\n",
    "            return 1\n",
    "    else:\n",
    "            return 0\n",
    "\n",
    "ft = udf(lambda x:filt(x))\n",
    "hw_tmp = hw.withColumn('filt',ft('cn_name'))\n",
    "hw_final = hw_tmp.filter(hw_tmp.filt == '1').select('mac','id','cn_name','m1','m2','m3','m4','m5','m6','m7','m8','m9','m10','m11','m12')\n",
    "\n",
    "hw_final = hw_final.withColumn('f1',conv(hw_final.m1, 16, 10)).withColumn('f2',conv(hw_final.m2, 16, 10))\\\n",
    "                         .withColumn('f3',conv(hw_final.m3, 16, 10)).withColumn('f4',conv(hw_final.m4, 16, 10))\\\n",
    "                         .withColumn('f5',conv(hw_final.m5, 16, 10)).withColumn('f6',conv(hw_final.m6, 16, 10))\\\n",
    "                         .withColumn('f7',conv(hw_final.m7, 16, 10)).withColumn('f8',conv(hw_final.m8, 16, 10))\\\n",
    "                         .withColumn('f9',conv(hw_final.m9, 16, 10)).withColumn('f10',conv(hw_final.m10, 16, 10))\\\n",
    "                         .withColumn('f11',conv(hw_final.m11, 16, 10)).withColumn('f12',conv(hw_final.m12, 16, 10))\n",
    "\n",
    "hw_final = hw_final.select('mac','id','cn_name',\\\n",
    "                            col('f1').cast('float'),col('f2').cast('float'),\\\n",
    "                            col('f3').cast('float'),col('f4').cast('float'),\\\n",
    "                            col('f5').cast('float'),col('f6').cast('float'),\\\n",
    "                            col('f7').cast('float'),col('f8').cast('float'),\\\n",
    "                            col('f9').cast('float'),col('f10').cast('float'),\\\n",
    "                            col('f11').cast('float'),col('f12').cast('float'))\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec = VectorAssembler(inputCols=['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12'],outputCol='features')\n",
    "new_df = vec.transform(hw_final)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "si = StringIndexer(inputCol='id', outputCol='label')\n",
    "si_model = si.fit(new_df)\n",
    "df_final = si_model.transform(new_df)\n",
    "\n",
    "fractions = df_final.select('id').distinct().withColumn('fraction', lit(0.7)).rdd.collectAsMap()\n",
    "trainDF = df_final.stat.sampleBy('id', fractions, seed=12)\n",
    "\n",
    "train_mac = trainDF.select('mac').withColumnRenamed('mac','train_mac')\n",
    "tmp = df_final.join(train_mac,df_final.mac==train_mac.train_mac,how='left')\n",
    "testDF = tmp.filter(tmp.train_mac.isNull())\n",
    "\n",
    "testDF = testDF.drop('train_mac')\n",
    "\n",
    "trainDF.write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/feature/trainDF_hw',compression='gzip')\n",
    "testDF.write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/feature/testDF_hw',compression='gzip')\n",
    "#df_final.write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/feature/all_features',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
