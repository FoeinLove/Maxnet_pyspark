{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering of recognized values (Label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_ct_res_hostname/*',sep='\\x01')\n",
    "d = d.withColumnRenamed('_c1','val').withColumnRenamed('_c3','osid')\n",
    "d = d.select('val','osid').dropna().distinct()\n",
    "#d1 = d.filter(d.osid == 0).distinct().withColumnRenamed('osid','label').select('val',col('label').cast('float'))\n",
    "d2 = d.filter(d.osid != 0).distinct().withColumn('label',lit(1)).select('val',col('label').cast('float')).distinct()\n",
    "\n",
    "#d2 = d2.sample(0.7)\n",
    "\n",
    "d2 = d2.withColumn('f1',length(col('val')))\n",
    "\n",
    "d2 = d2.withColumn('f2',when(d2.val.startswith('A')|d2.val.startswith('B')|d2.val.startswith('C')\\\n",
    "                             |d2.val.startswith('D')|d2.val.startswith('E')|d2.val.startswith('F')\\\n",
    "                             |d2.val.startswith('G')|d2.val.startswith('H')|d2.val.startswith('I')\\\n",
    "                             |d2.val.startswith('J')|d2.val.startswith('K')|d2.val.startswith('L')\\\n",
    "                             |d2.val.startswith('M')|d2.val.startswith('N')|d2.val.startswith('O')\\\n",
    "                             |d2.val.startswith('P')|d2.val.startswith('Q')|d2.val.startswith('R')\\\n",
    "                             |d2.val.startswith('S')|d2.val.startswith('T')|d2.val.startswith('U')\\\n",
    "                             |d2.val.startswith('V')|d2.val.startswith('W')|d2.val.startswith('X')\\\n",
    "                             |d2.val.startswith('Y')|d2.val.startswith('Z'),1).otherwise(0))\n",
    "\n",
    "import re\n",
    "\n",
    "num_regex = re.compile(r'[0-9]') #数字\n",
    "xiaoxiezimu_regex = re.compile(r'[a-z]')#小写字母\n",
    "daxiezimu_regex = re.compile(r'[A-Z]') #大写字母 \n",
    "#hanzi_regex = re.compile(r'[\\u4E00-\\u9FA5]')#汉字\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "num = udf(lambda x: len(num_regex.findall(x)))\n",
    "xiaoxie = udf(lambda x: len(xiaoxiezimu_regex.findall(x)))\n",
    "daxie = udf(lambda x: len(daxiezimu_regex.findall(x)))\n",
    "\n",
    "d2 = d2.withColumn('f3',num('val'))\n",
    "d2 = d2.withColumn('f4',xiaoxie('val'))\n",
    "d2 = d2.withColumn('f5',daxie('val'))\n",
    "\n",
    "# 特征字符串长度 f1\n",
    "# 首字母是否大写 f2\n",
    "# 数字字符数量   f3\n",
    "# 小写字母数量   f4\n",
    "# 大写字母数量   f5\n",
    "# 特殊符号-_:的数量 f6\n",
    "# 空格的数量 f7\n",
    "\n",
    "\n",
    "# 统计下划线个数\n",
    "def xiahuaxian_count(s):\n",
    "    xiahuaxian_counts=0\n",
    "    for c in s:\n",
    "        xiahuaxian_split_list = c.split('_')\n",
    "        xiahuaxian_counts += len(xiahuaxian_split_list) - 1\n",
    "    return xiahuaxian_counts\n",
    "\n",
    "# 统计中划线个数\n",
    "def zhonghuaxian_count(s):\n",
    "    zhonghuaxian_counts=0\n",
    "    for c in s:\n",
    "        zhonghuaxian_split_list = c.split('-')\n",
    "        zhonghuaxian_counts += len(zhonghuaxian_split_list) - 1\n",
    "    return zhonghuaxian_counts\n",
    "\n",
    "# 统计冒号个数\n",
    "def maohao_count(s):\n",
    "    maohao_counts=0\n",
    "    for c in s:\n",
    "        maohao_split_list = c.split(':')\n",
    "        maohao_counts += len(maohao_split_list) - 1\n",
    "    return maohao_counts\n",
    "\n",
    "def teshu_count(s):\n",
    "    teshu_counts=0\n",
    "    a_counts=0\n",
    "    b_counts=0\n",
    "    c_counts=0\n",
    "    for c in s:\n",
    "        a_split_list = c.split('_')\n",
    "        a_counts += len(a_split_list) - 1\n",
    "        \n",
    "        b_split_list = c.split('-')\n",
    "        b_counts += len(b_split_list) - 1\n",
    "        \n",
    "        c_split_list = c.split(':')\n",
    "        c_counts += len(c_split_list) - 1\n",
    "        \n",
    "        teshu_counts = a_counts + b_counts + c_counts\n",
    "    return teshu_counts\n",
    "        \n",
    "\n",
    "# 统计空格个数\n",
    "def space_count(s):\n",
    "    space_counts=0\n",
    "    for c in s:\n",
    "        space_split_list = c.split(' ')\n",
    "        space_counts += len(space_split_list) - 1\n",
    "    return space_counts\n",
    "\n",
    "teshu = udf(lambda x: teshu_count(x))\n",
    "kongge = udf(lambda x: space_count(x))\n",
    "\n",
    "\n",
    "d2 = d2.withColumn('f6',teshu('val'))\n",
    "d2 = d2.withColumn('f7',kongge('val'))\n",
    "\n",
    "d2 = d2.select('val','label',col('f1').cast('float'),col('f2').cast('float'),col('f3').cast('float'),col('f4').cast('float'),col('f5').cast('float'),col('f6').cast('float'),col('f7').cast('float'))\n",
    "\n",
    "#dd = d2.repartition(1)\n",
    "\n",
    "#dd.write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/feature/recognized/',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering of unrecognized values (Label == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_ct_res_hostname/*',sep='\\x01')\n",
    "d = d.withColumnRenamed('_c1','val').withColumnRenamed('_c3','osid')\n",
    "d = d.select('val','osid').dropna().distinct()\n",
    "d1 = d.filter(d.osid == 0).distinct().withColumnRenamed('osid','label').select('val',col('label').cast('float'))\n",
    "#d2 = d.filter(d.osid != 0).distinct().withColumn('label',lit(1)).select('val',col('label').cast('float')).distinct()\n",
    "\n",
    "#d2 = d2.sample(0.7)\n",
    "\n",
    "d1 = d1.withColumn('f1',length(col('val')))\n",
    "\n",
    "d1 = d1.withColumn('f2',when(d1.val.startswith('A')|d1.val.startswith('B')|d1.val.startswith('C')\\\n",
    "                             |d1.val.startswith('D')|d1.val.startswith('E')|d1.val.startswith('F')\\\n",
    "                             |d1.val.startswith('G')|d1.val.startswith('H')|d1.val.startswith('I')\\\n",
    "                             |d1.val.startswith('J')|d1.val.startswith('K')|d1.val.startswith('L')\\\n",
    "                             |d1.val.startswith('M')|d1.val.startswith('N')|d1.val.startswith('O')\\\n",
    "                             |d1.val.startswith('P')|d1.val.startswith('Q')|d1.val.startswith('R')\\\n",
    "                             |d1.val.startswith('S')|d1.val.startswith('T')|d1.val.startswith('U')\\\n",
    "                             |d1.val.startswith('V')|d1.val.startswith('W')|d1.val.startswith('X')\\\n",
    "                             |d1.val.startswith('Y')|d1.val.startswith('Z'),1).otherwise(0))\n",
    "\n",
    "import re\n",
    "\n",
    "num_regex = re.compile(r'[0-9]') #数字\n",
    "xiaoxiezimu_regex = re.compile(r'[a-z]')#小写字母\n",
    "daxiezimu_regex = re.compile(r'[A-Z]') #大写字母 \n",
    "#hanzi_regex = re.compile(r'[\\u4E00-\\u9FA5]')#汉字\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "num = udf(lambda x: len(num_regex.findall(x)))\n",
    "xiaoxie = udf(lambda x: len(xiaoxiezimu_regex.findall(x)))\n",
    "daxie = udf(lambda x: len(daxiezimu_regex.findall(x)))\n",
    "\n",
    "d1 = d1.withColumn('f3',num('val'))\n",
    "d1 = d1.withColumn('f4',xiaoxie('val'))\n",
    "d1 = d1.withColumn('f5',daxie('val'))\n",
    "\n",
    "# 特征字符串长度 f1\n",
    "# 首字母是否大写 f2\n",
    "# 数字字符数量   f3\n",
    "# 小写字母数量   f4\n",
    "# 大写字母数量   f5\n",
    "# 特殊符号-_:的数量 f6\n",
    "# 空格的数量 f7\n",
    "\n",
    "\n",
    "# 统计下划线个数\n",
    "def xiahuaxian_count(s):\n",
    "    xiahuaxian_counts=0\n",
    "    for c in s:\n",
    "        xiahuaxian_split_list = c.split('_')\n",
    "        xiahuaxian_counts += len(xiahuaxian_split_list) - 1\n",
    "    return xiahuaxian_counts\n",
    "\n",
    "# 统计中划线个数\n",
    "def zhonghuaxian_count(s):\n",
    "    zhonghuaxian_counts=0\n",
    "    for c in s:\n",
    "        zhonghuaxian_split_list = c.split('-')\n",
    "        zhonghuaxian_counts += len(zhonghuaxian_split_list) - 1\n",
    "    return zhonghuaxian_counts\n",
    "\n",
    "# 统计冒号个数\n",
    "def maohao_count(s):\n",
    "    maohao_counts=0\n",
    "    for c in s:\n",
    "        maohao_split_list = c.split(':')\n",
    "        maohao_counts += len(maohao_split_list) - 1\n",
    "    return maohao_counts\n",
    "\n",
    "def teshu_count(s):\n",
    "    teshu_counts=0\n",
    "    a_counts=0\n",
    "    b_counts=0\n",
    "    c_counts=0\n",
    "    for c in s:\n",
    "        a_split_list = c.split('_')\n",
    "        a_counts += len(a_split_list) - 1\n",
    "        \n",
    "        b_split_list = c.split('-')\n",
    "        b_counts += len(b_split_list) - 1\n",
    "        \n",
    "        c_split_list = c.split(':')\n",
    "        c_counts += len(c_split_list) - 1\n",
    "        \n",
    "        teshu_counts = a_counts + b_counts + c_counts\n",
    "    return teshu_counts\n",
    "        \n",
    "\n",
    "# 统计空格个数\n",
    "def space_count(s):\n",
    "    space_counts=0\n",
    "    for c in s:\n",
    "        space_split_list = c.split(' ')\n",
    "        space_counts += len(space_split_list) - 1\n",
    "    return space_counts\n",
    "\n",
    "teshu = udf(lambda x: teshu_count(x))\n",
    "kongge = udf(lambda x: space_count(x))\n",
    "\n",
    "\n",
    "d1 = d1.withColumn('f6',teshu('val'))\n",
    "d1 = d1.withColumn('f7',kongge('val'))\n",
    "\n",
    "\n",
    "d1 = d1.select('val','label',col('f1').cast('float'),col('f2').cast('float'),col('f3').cast('float'),col('f4').cast('float'),col('f5').cast('float'),col('f6').cast('float'),col('f7').cast('float'))\n",
    "\n",
    "#ddd = d1.repartition(1)\n",
    "\n",
    "#ddd.write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/feature/unrecognized/',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "d1 = spark.read.parquet('/data/user/hive/warehouse/ian/feature/unrecognized/*')\n",
    "d2 = spark.read.parquet('/data/user/hive/warehouse/ian/feature/recognized/*')\n",
    "\n",
    "#d1 = d1.withColumn('id1',monotonically_increasing_id())\n",
    "#d2 = d2.withColumn('id2',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13366199, 118581095)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.count(),d2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131947294"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13366199+118581095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val', 'label', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val', 'label', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = d1.union(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131947294"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4034425936156893"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.corr('label','f1') ## 特征字符串长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07627069092079923"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.corr('label','f2') # 首字母是否大写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07186411864088836"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.corr('label','f3') # 数字字符数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3887611231602581"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.corr('label','f4') ## 小写字母数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19934939410810917"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.corr('label','f5') # 大写字母数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3214068297187456"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.corr('label','f6') ## 特殊符号-_:的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01872019384940059"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.corr('label','f7') # 空格的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                f1|\n",
      "+-------+------------------+\n",
      "|  count|          13366199|\n",
      "|   mean|16.745152081006726|\n",
      "| stddev|  8.51290374526239|\n",
      "|    min|               1.0|\n",
      "|    max|              64.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1.describe('f1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|               f1|\n",
      "+-------+-----------------+\n",
      "|  count|        118581095|\n",
      "|   mean| 22.8576718067918|\n",
      "| stddev|3.361524311119653|\n",
      "|    min|              3.0|\n",
      "|    max|             64.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2.describe('f1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|               f4|\n",
      "+-------+-----------------+\n",
      "|  count|         13366199|\n",
      "|   mean|3.770518604428978|\n",
      "| stddev|5.273092558697898|\n",
      "|    min|              0.0|\n",
      "|    max|             64.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1.describe('f4').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                f4|\n",
      "+-------+------------------+\n",
      "|  count|         118581095|\n",
      "|   mean|10.352725339566142|\n",
      "| stddev| 4.638554647787675|\n",
      "|    min|               0.0|\n",
      "|    max|              62.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2.describe('f4').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                f6|\n",
      "+-------+------------------+\n",
      "|  count|          13366199|\n",
      "|   mean|0.4301927571181605|\n",
      "| stddev|0.8938209331999687|\n",
      "|    min|               0.0|\n",
      "|    max|              58.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1.describe('f6').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                f6|\n",
      "+-------+------------------+\n",
      "|  count|         118581095|\n",
      "|   mean|1.2792700893848215|\n",
      "| stddev|0.7374773238050477|\n",
      "|    min|               0.0|\n",
      "|    max|              39.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2.describe('f6').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "衍生特征与是否能识别的相关性量化指标（从大到小排序）：\n",
    "1.字符串长度：0.403\n",
    "2.小写字母数量：0.389\n",
    "3.特殊符号-_:的数量：0.321\n",
    "4.数字字符数量： 0.072\n",
    "5.空格的数量： -0.019\n",
    "6.首字母是否大写：-0.076\n",
    "7.大写字母数量： -0.199\n",
    "\n",
    "从统计学角度来说，相关系数最高的0.403也不能说是很强的相关性（一般强相关>=0.7），只能说弱相关。但同时考虑到数据源的随机性较高，目前可以考虑将特征字符串长度，小写字母数量及特征符号数量纳入特征识别的二次删选标准里面来。\n",
    "\n",
    "此处需要注意，以上给出的只是根据俊安经验总结出来的潜在特征与是否能被识别（标签）之间的量化关系，并不能够给出具体的诸如字符串长度为多少时，就能被识别；小写字母数量为多少时，就能被识别；特殊符号的数量为多少时，就能被识别。但可以给出带有一定概率的统计结果来辅助判断特征识别，一定程度上提高混在未识别特征中的潜在可识别特征的概率：\n",
    "\n",
    "目前可以给出的辅助判断条件有：\n",
    "1. 特征字符串长度不超过16时，能被识别的概率相对较高；字符串长度超过22时，不能被识别的概率相对较高。\n",
    "2. 数字字符数量不超过3时，能被识别的概率相对较高；数量超过10时，不能被识别的概率相对较高。\n",
    "3. 不含特殊符号时，能被识别的概率相对较高；当特殊符号数量超过2个时，不能被识别的概率相对较高。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7132714610776969"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_ct_res_hostname/*',sep='\\x01')\n",
    "\n",
    "d = d.withColumnRenamed('_c1','val').withColumnRenamed('_c3','osid')\n",
    "d = d.select('val','osid')\n",
    "\n",
    "d1 = d.filter(d.osid == 0).withColumnRenamed('osid','label').select('val',col('label').cast('float')).dropna()\n",
    "d2 = d.filter(d.osid != 0).withColumn('label',lit(1)).select('val',col('label').cast('float')).dropna()\n",
    "\n",
    "df = d1.union(d2)\n",
    "\n",
    "g = df.groupBy('val').count().withColumnRenamed('val','val1')\n",
    "\n",
    "dd = df.join(g,df.val == g.val1,how='left_outer')\n",
    "\n",
    "dd = dd.withColumnRenamed('count','val_count')\n",
    "\n",
    "dd.corr('label','val_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|val_count         |\n",
      "+-------+------------------+\n",
      "|count  |3525              |\n",
      "|mean   |40473.719432624115|\n",
      "|stddev |291219.4611998903 |\n",
      "|min    |1                 |\n",
      "|max    |14196282          |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.filter(dd.label == 1).select('val_count').distinct().describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|val_count        |\n",
      "+-------+-----------------+\n",
      "|count  |1519             |\n",
      "|mean   |33510.17577353522|\n",
      "|stddev |831804.8084261566|\n",
      "|min    |1                |\n",
      "|max    |29398040         |\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.filter(dd.label == 0).select('val_count').distinct().describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "d1 = spark.read.parquet('/data/user/hive/warehouse/ian/feature/unrecognized/*')\n",
    "d2 = spark.read.parquet('/data/user/hive/warehouse/ian/feature/recognized/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d1.union(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "unlist = udf(lambda x: float(list(x)[0]), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler,StandardScaler,VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| stddev_samp(mm_f1)|\n",
      "+-------------------+\n",
      "|0.07982150640018788|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_f1 = VectorAssembler(inputCols=['f4'],outputCol='vec_f1')\n",
    "f1_v = vec_f1.transform(df)\n",
    "minmax_f1 = MinMaxScaler(inputCol='vec_f1',outputCol='minmax_f1')\n",
    "minmax = minmax_f1.fit(f1_v)\n",
    "mm_f1 = minmax.transform(f1_v).select('minmax_f1').withColumn('mm_f1',unlist('minmax_f1')).select('mm_f1')\n",
    "mm_f1.select(stddev('mm_f1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| stddev_samp(mm_f1)|\n",
      "+-------------------+\n",
      "|0.05313065963090275|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_f1 = VectorAssembler(inputCols=['f5'],outputCol='vec_f1')\n",
    "f1_v = vec_f1.transform(df)\n",
    "minmax_f1 = MinMaxScaler(inputCol='vec_f1',outputCol='minmax_f1')\n",
    "minmax = minmax_f1.fit(f1_v)\n",
    "mm_f1 = minmax.transform(f1_v).select('minmax_f1').withColumn('mm_f1',unlist('minmax_f1')).select('mm_f1')\n",
    "mm_f1.select(stddev('mm_f1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| stddev_samp(mm_f1)|\n",
      "+-------------------+\n",
      "|0.01374280408348266|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_f1 = VectorAssembler(inputCols=['f6'],outputCol='vec_f1')\n",
    "f1_v = vec_f1.transform(df)\n",
    "minmax_f1 = MinMaxScaler(inputCol='vec_f1',outputCol='minmax_f1')\n",
    "minmax = minmax_f1.fit(f1_v)\n",
    "mm_f1 = minmax.transform(f1_v).select('minmax_f1').withColumn('mm_f1',unlist('minmax_f1')).select('mm_f1')\n",
    "mm_f1.select(stddev('mm_f1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  stddev_samp(mm_f1)|\n",
      "+--------------------+\n",
      "|0.001091062901651...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_f1 = VectorAssembler(inputCols=['f7'],outputCol='vec_f1')\n",
    "f1_v = vec_f1.transform(df)\n",
    "minmax_f1 = MinMaxScaler(inputCol='vec_f1',outputCol='minmax_f1')\n",
    "minmax = minmax_f1.fit(f1_v)\n",
    "mm_f1 = minmax.transform(f1_v).select('minmax_f1').withColumn('mm_f1',unlist('minmax_f1')).select('mm_f1')\n",
    "mm_f1.select(stddev('mm_f1')).show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f1_stddev: 0.07256198926022006\n",
    "f2_stddev: 0.4860337267524121\n",
    "f3_stddev: 0.0648818264012149\n",
    "f4_stddev: 0.07982150640018788\n",
    "f5_stddev: 0.05313065963090275\n",
    "f6_stddev: 0.01374280408348266\n",
    "f7_stddev: 0.001091062901651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
