{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crontab script (load model -> predict -> write to HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "m = RandomForestClassificationModel.load('/data/user/hive/warehouse/ian/model/m')\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_visual_unknown/*',sep='\\x01')\n",
    "d1 = d.select('_c2').withColumnRenamed('_c2','val').distinct().dropna()\n",
    "\n",
    "d1 = d1.withColumn('f1',length(col('val')))\n",
    "\n",
    "d1 = d1.withColumn('f2',when(d1.val.startswith('A')|d1.val.startswith('B')|d1.val.startswith('C')\\\n",
    "                             |d1.val.startswith('D')|d1.val.startswith('E')|d1.val.startswith('F')\\\n",
    "                             |d1.val.startswith('G')|d1.val.startswith('H')|d1.val.startswith('I')\\\n",
    "                             |d1.val.startswith('J')|d1.val.startswith('K')|d1.val.startswith('L')\\\n",
    "                             |d1.val.startswith('M')|d1.val.startswith('N')|d1.val.startswith('O')\\\n",
    "                             |d1.val.startswith('P')|d1.val.startswith('Q')|d1.val.startswith('R')\\\n",
    "                             |d1.val.startswith('S')|d1.val.startswith('T')|d1.val.startswith('U')\\\n",
    "                             |d1.val.startswith('V')|d1.val.startswith('W')|d1.val.startswith('X')\\\n",
    "                             |d1.val.startswith('Y')|d1.val.startswith('Z'),1).otherwise(0))\n",
    "\n",
    "import re\n",
    "\n",
    "num_regex = re.compile(r'[0-9]') \n",
    "xiaoxiezimu_regex = re.compile(r'[a-z]')\n",
    "daxiezimu_regex = re.compile(r'[A-Z]')\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "num = udf(lambda x: len(num_regex.findall(x)))\n",
    "xiaoxie = udf(lambda x: len(xiaoxiezimu_regex.findall(x)))\n",
    "daxie = udf(lambda x: len(daxiezimu_regex.findall(x)))\n",
    "\n",
    "d1 = d1.withColumn('f3',num('val'))\n",
    "d1 = d1.withColumn('f4',xiaoxie('val'))\n",
    "d1 = d1.withColumn('f5',daxie('val'))\n",
    "\n",
    "def xiahuaxian_count(s):\n",
    "    xiahuaxian_counts=0\n",
    "    for c in s:\n",
    "        xiahuaxian_split_list = c.split('_')\n",
    "        xiahuaxian_counts += len(xiahuaxian_split_list) - 1\n",
    "    return xiahuaxian_counts\n",
    "\n",
    "\n",
    "def zhonghuaxian_count(s):\n",
    "    zhonghuaxian_counts=0\n",
    "    for c in s:\n",
    "        zhonghuaxian_split_list = c.split('-')\n",
    "        zhonghuaxian_counts += len(zhonghuaxian_split_list) - 1\n",
    "    return zhonghuaxian_counts\n",
    "\n",
    "def maohao_count(s):\n",
    "    maohao_counts=0\n",
    "    for c in s:\n",
    "        maohao_split_list = c.split(':')\n",
    "        maohao_counts += len(maohao_split_list) - 1\n",
    "    return maohao_counts\n",
    "\n",
    "def teshu_count(s):\n",
    "    teshu_counts=0\n",
    "    a_counts=0\n",
    "    b_counts=0\n",
    "    c_counts=0\n",
    "    for c in s:\n",
    "        a_split_list = c.split('_')\n",
    "        a_counts += len(a_split_list) - 1\n",
    "        \n",
    "        b_split_list = c.split('-')\n",
    "        b_counts += len(b_split_list) - 1\n",
    "        \n",
    "        c_split_list = c.split(':')\n",
    "        c_counts += len(c_split_list) - 1\n",
    "        \n",
    "        teshu_counts = a_counts + b_counts + c_counts\n",
    "    return teshu_counts\n",
    "        \n",
    "def space_count(s):\n",
    "    space_counts=0\n",
    "    for c in s:\n",
    "        space_split_list = c.split(' ')\n",
    "        space_counts += len(space_split_list) - 1\n",
    "    return space_counts\n",
    "\n",
    "teshu = udf(lambda x: teshu_count(x))\n",
    "kongge = udf(lambda x: space_count(x))\n",
    "\n",
    "\n",
    "d1 = d1.withColumn('f6',teshu('val'))\n",
    "d1 = d1.withColumn('f7',kongge('val'))\n",
    "\n",
    "\n",
    "d1 = d1.select('val',col('f1').cast('float'),\\\n",
    "               col('f2').cast('float'),\\\n",
    "               col('f3').cast('float'),\\\n",
    "               col('f4').cast('float'),\\\n",
    "               col('f5').cast('float'),\\\n",
    "               col('f6').cast('float'),\\\n",
    "               col('f7').cast('float'))\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vec = VectorAssembler(inputCols=['f1','f2','f3','f4','f5','f6','f7'],outputCol='features')\n",
    "unknow = vec.transform(d1)\n",
    "\n",
    "t = m.transform(unknow)\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "unlist = udf(lambda x: float(list(x)[1]), DoubleType())\n",
    "\n",
    "total = t.select('val',unlist('probability').alias('probability'),'prediction')\n",
    "\n",
    "def filt(s):\n",
    "    l = ['unknown','empty','NONE','none','N/A','normal','anonymous','null','AUTOBVT','mysimplelink','deep-20','fyyx-20','MICROSO-','PB-SmartPower','windows','WINMICR-','generic_']\n",
    "    for x in l:\n",
    "        if x in s:\n",
    "            return 1\n",
    "    else:\n",
    "            return 0\n",
    "        \n",
    "ft = udf(lambda x:filt(x))\n",
    "\n",
    "k = total.withColumn('filt',ft('val'))\n",
    "kk = k.filter(k.filt == 0).select('val','probability','prediction')\n",
    "\n",
    "kkk = kk.filter(~kk.val.rlike('-20\\d{6}'))\\\n",
    "        .filter(~kk.val.rlike('^admin'))\\\n",
    "        .filter(~kk.val.rlike('^china'))\\\n",
    "        .filter(~kk.val.rlike('SC-[A-Z]{4}\\d{4}\\w{4}'))\\\n",
    "        .filter(~kk.val.rlike('^XYXS-20'))\\\n",
    "        .filter(~kk.val.rlike('20\\d{10}'))\\\n",
    "        .filter(~kk.val.rlike('^YOS-'))\\\n",
    "        .filter(~kk.val.rlike('^WWW-'))\\\n",
    "        .filter(~kk.val.rlike('^LUKA-'))\\\n",
    "        .filter(~kk.val.rlike('^LRGM-'))\n",
    "\n",
    "#total_hdfs = kkk.repartition(1)\n",
    "\n",
    "#total_hdfs.write.mode('overwrite').csv('hdfs:///user/maxnet/data/prediction/p_20190917.csv',header=True,compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
