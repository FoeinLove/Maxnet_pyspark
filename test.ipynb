{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring,conv,col,min,max,udf,lit\n",
    "   \n",
    "d1 = spark.read.csv('/user/maxnet/database/sig.db/term_desc_all',sep='\\x01')\n",
    "d1 = d1.withColumnRenamed('_c0','term_id')\\\n",
    "       .withColumnRenamed('_c1','brand')\\\n",
    "       .withColumnRenamed('_c2','cn_name')\\\n",
    "       .withColumnRenamed('_c3','en_name')\\\n",
    "       .withColumnRenamed('_c4','type1')\\\n",
    "       .withColumnRenamed('_c5','type2')\\\n",
    "       .withColumnRenamed('_c6','os')\\\n",
    "       .withColumnRenamed('_c7','dtime')\\\n",
    "       .withColumnRenamed('_c8','price')\\\n",
    "       .withColumnRenamed('_c9','remarks')\\\n",
    "       .withColumnRenamed('_c10','prio')\n",
    "\n",
    "d2 = spark.read.csv('/user/maxnet/database/sig.db/term_oui',sep='\\x01')\n",
    "d2 = d2.withColumnRenamed('_c0','mac_6').withColumnRenamed('_c1','manu')\n",
    "\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_mac_res',sep='\\x01')\n",
    "d = d.withColumnRenamed('_c0','mac').withColumnRenamed('_c1','id')\n",
    "d = d.withColumn('m1_6',substring(d.mac,1,6))\n",
    "\n",
    "tmp_1 = d.join(d2,d.m1_6==d2.mac_6)\n",
    "df = tmp_1.join(d1,tmp_1.id == d1.term_id).select('mac','id','manu','brand','prio').distinct().dropna()\n",
    "df = df.filter(df.prio == 4).select('mac','id','brand').distinct().dropna()\n",
    "df = df.filter(df.brand == '小米')\n",
    "\n",
    "df = df.withColumn('m1',substring(df.mac,1,6)).withColumn('m2',substring(df.mac,7,6))\n",
    "df = df.withColumn('p',conv(df.m2, 16, 10))\n",
    "df = df.select('mac','id','m1','m2',col('p').cast('bigint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|    m1|  count|\n",
      "+------+-------+\n",
      "|9487E0|1217740|\n",
      "|9C2EA1| 782031|\n",
      "|F4F5DB| 712422|\n",
      "|A45046| 697953|\n",
      "|4C49E3| 663933|\n",
      "|80AD16| 634729|\n",
      "|482CA0| 623304|\n",
      "|D86375| 610537|\n",
      "|38A4ED| 598649|\n",
      "|B0E235| 589208|\n",
      "|ACC1EE| 575838|\n",
      "|742344| 567940|\n",
      "|508F4C| 559196|\n",
      "|00EC0A| 535711|\n",
      "|ECD09F| 516090|\n",
      "|7802F8| 514343|\n",
      "|E446DA| 504217|\n",
      "|50642B| 500685|\n",
      "|2047DA| 489869|\n",
      "|F460E2| 478567|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('m1').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = df.filter(df.m1 == '9487E0')\n",
    "d2 = df.filter(df.m1 == '9C2EA1')\n",
    "d3 = df.filter(df.m1 == 'F4F5DB')\n",
    "d4 = df.filter(df.m1 == 'A45046')\n",
    "d5 = df.filter(df.m1 == '4C49E3')\n",
    "d6 = df.filter(df.m1 == '80AD16')\n",
    "d7 = df.filter(df.m1 == '482CA0')\n",
    "d8 = df.filter(df.m1 == 'D86375')\n",
    "d9 = df.filter(df.m1 == '38A4ED')\n",
    "d10 = df.filter(df.m1 == 'B0E235')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d1.sample(0.01).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d1',compression='gzip')\n",
    "#d2.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d2',compression='gzip')\n",
    "#d3.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d3',compression='gzip')\n",
    "#d4.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d4',compression='gzip')\n",
    "#d5.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d5',compression='gzip')\n",
    "#d6.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d6',compression='gzip')\n",
    "#d7.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d7',compression='gzip')\n",
    "#d8.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d8',compression='gzip')\n",
    "#d9.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d9',compression='gzip')\n",
    "#d10.sample(0.1).write.mode('overwrite').parquet('hdfs:///data/user/hive/warehouse/ian/d10',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    \n",
    "    from pyspark.sql.functions import substring,conv,col,min,max,udf,lit\n",
    "   \n",
    "    d3 = spark.read.parquet('/data/user/hive/warehouse/ian/tmp_9C2EA1')\n",
    "    \n",
    "    bottom = d3.select(min('p')).collect()[0]['min(p)']\n",
    "    top = d3.select(max('p')).collect()[0]['max(p)'] \n",
    "    \n",
    "    if x < bottom:\n",
    "        return d3.filter(d3.p == bottom).select('id').collect()[0]['p']\n",
    "    \n",
    "    elif x > top:\n",
    "        return d3.filter(d3.p == top).select('id').collect()[0]['p']\n",
    "    \n",
    "    else :\n",
    "        tmp1 = d3.where(col('p').between((x-10),(x+10))).groupBy('id').count()\n",
    "        tmp2 = d3.where(col('p').between((x-100),(x+100))).groupBy('id').count()\n",
    "        tmp3 = d3.where(col('p').between((x-1000),(x+1000))).groupBy('id').count()\n",
    "        tmp4 = d3.where(col('p').between((x-10000),(x+10000))).groupBy('id').count()\n",
    "        tmp5 = d3.where(col('p').between((x-100000),(x+100000))).groupBy('id').count()\n",
    "        return tmp1.union(tmp2).union(tmp3).union(tmp4).union(tmp5).groupBy('id').count().sort('count',ascending=False).collect()[0]['id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import substring,conv,col\n",
    "\n",
    "#test = test.withColumn('m1',substring(test.mac,1,6)).withColumn('m2',substring(test.mac,7,6))\n",
    "#test = test.withColumn('p',conv(test.m2, 16, 10))  \n",
    "#test = test.select('mac','id','m1','m2',col('p').cast('bigint'))\n",
    "test = spark.read.parquet('/data/user/hive/warehouse/ian/d2')\n",
    "\n",
    "result = []\n",
    "a = test.select('p').toPandas()['p'].values.tolist()\n",
    "for x in a:\n",
    "    result.append(predict(x))\n",
    "\n",
    "mac = test.select('mac').toPandas()['mac'].values.tolist()\n",
    "pre = spark.createDataFrame(pd.DataFrame({'mac':mac,'predict_id':result}))\n",
    "\n",
    "pre = pre.withColumnRenamed('mac','mac1')\n",
    "dd = test.join(pre,test.mac == pre.mac1,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "dd = dd.withColumn('equal',when(dd.predict_id == dd.id, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|equal|count|\n",
      "+-----+-----+\n",
      "|    1|72616|\n",
      "|    0| 5622|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.groupBy('equal').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9281423349267619"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "72616/(5622+72616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d1|9487E0|1217740|99%|\n",
    "d2|9C2EA1| 782031|92%|\n",
    "d3|F4F5DB| 712422|\n",
    "d4|A45046| 697953|\n",
    "d5|4C49E3| 663933|\n",
    "d6|80AD16| 634729|\n",
    "d7|482CA0| 623304|\n",
    "d8|D86375| 610537|\n",
    "d9|38A4ED| 598649|\n",
    "d10|B0E235| 589208|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
