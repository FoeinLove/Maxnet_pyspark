{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    from pyspark.sql.functions import substring,conv,col,min,max,udf,lit\n",
    "   \n",
    "    d1 = spark.read.csv('/user/maxnet/database/sig.db/term_desc_all',sep='\\x01')\n",
    "    d1 = d1.withColumnRenamed('_c0','term_id')\\\n",
    "           .withColumnRenamed('_c1','brand')\\\n",
    "           .withColumnRenamed('_c2','cn_name')\\\n",
    "           .withColumnRenamed('_c3','en_name')\\\n",
    "           .withColumnRenamed('_c4','type1')\\\n",
    "           .withColumnRenamed('_c5','type2')\\\n",
    "           .withColumnRenamed('_c6','os')\\\n",
    "           .withColumnRenamed('_c7','dtime')\\\n",
    "           .withColumnRenamed('_c8','price')\\\n",
    "           .withColumnRenamed('_c9','remarks')\\\n",
    "           .withColumnRenamed('_c10','prio')\n",
    "\n",
    "    d2 = spark.read.csv('/user/maxnet/database/sig.db/term_oui',sep='\\x01')\n",
    "    d2 = d2.withColumnRenamed('_c0','mac_6').withColumnRenamed('_c1','manu')\n",
    "    \n",
    "    d = spark.read.csv('/user/maxnet/database/sig.db/data_mac_res',sep='\\x01')\n",
    "    d = d.withColumnRenamed('_c0','mac').withColumnRenamed('_c1','id')\n",
    "    d = d.withColumn('m1_6',substring(d.mac,1,6))\n",
    "\n",
    "    tmp_1 = d.join(d2,d.m1_6==d2.mac_6)\n",
    "    df = tmp_1.join(d1,tmp_1.id == d1.term_id).select('mac','id','manu','brand','prio').distinct().dropna()\n",
    "    df = df.filter(df.prio == 4).select('mac','id','brand').distinct().dropna()\n",
    "    \n",
    "    df = df.withColumn('m1',substring(df.mac,1,6)).withColumn('m2',substring(df.mac,7,6))\n",
    "    df = df.withColumn('p',conv(df.m2, 16, 10))\n",
    "    df = df.select('mac','id','m1','m2',col('p').cast('bigint'))\n",
    "    d1 = df.filter(df.m1 =='24DF6A')\n",
    "    \n",
    "    gp_id_pmin = d1.groupBy('id').agg({'p':'min'})\n",
    "    gp_id_pmax = d1.groupBy('id').agg({'p':'max'}).withColumnRenamed('id','id1')\n",
    "    gp_id_count = d1.groupBy('id').count().withColumnRenamed('id','id2')\n",
    "\n",
    "    stat = gp_id_pmin.join(gp_id_pmax,gp_id_pmin.id ==gp_id_pmax.id1)\n",
    "    stat = stat.join(gp_id_count,stat.id == gp_id_count.id2)\n",
    "    stat = stat.select('id','min(p)','max(p)','count')\n",
    "    stat = stat.withColumn('effect_density',col('count')/(col('max(p)')-col('min(p)')))\n",
    "    stat = stat.fillna(0,'effect_density')\n",
    "    \n",
    "    \n",
    "    bottom = d1.select(min('p')).collect()[0]['min(p)']\n",
    "    top = d1.select(max('p')).collect()[0]['max(p)']\n",
    "    \n",
    "    base = stat.withColumn('x-min(p)',lit(x)-col('min(p)'))\\\n",
    "           .withColumn('x-max(p)',lit(x)-col('max(p)'))\n",
    "\n",
    "    base = base.withColumn('mul',col('x-min(p)')*col('x-max(p)'))\n",
    "\n",
    "    position = udf(lambda x: 0 if x > 0 else 1)\n",
    "\n",
    "    base = base.withColumn('position',position('mul'))\n",
    "    base = base.select('id','min(p)','max(p)','count','effect_density','position').toPandas()   \n",
    "    \n",
    "    if x < bottom:\n",
    "        return base[base['min(p)'] == bottom].iloc[0]['id']\n",
    "    \n",
    "    elif x > top:\n",
    "        return base[base['min(p)'] == top].iloc[0]['id']\n",
    "    \n",
    "    else :\n",
    "        max_effect = base[base.position == '1']['effect_density'].max()\n",
    "        return base[base.effect_density == max_effect].iloc[0]['id']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/session.py:357: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import substring,conv,lit,col\n",
    "test1 = [{'mac':'24DF6A0018CA'}]\n",
    "test2 = [{'mac':'24DF6A0018CE'}]\n",
    "inp1 = spark.createDataFrame(test1)\n",
    "inp2 = spark.createDataFrame(test2)\n",
    "inp = inp1.union(inp2)\n",
    "inp = inp.withColumn('m1',substring(inp.mac,1,6)).withColumn('m2',substring(inp.mac,7,6)).withColumn('id',lit(None).cast(StringType()))\n",
    "inp = inp.withColumn('p',conv(inp.m2, 16, 10))  \n",
    "inp = inp.select('mac','m1','m2',col('p').cast('bigint'),'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for x in inp.select('p').toPandas()['p'].values.tolist():\n",
    "    result.append(predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['40316', '40316']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|         mac|   id|\n",
      "+------------+-----+\n",
      "|24DF6A0018CA|40316|\n",
      "|24DF6A0018CE|40316|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mac = inp.select('mac').toPandas()['mac'].values.tolist()\n",
    "spark.createDataFrame(pd.DataFrame({'mac':mac,'id':result})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring,conv,col,min,max,udf,lit\n",
    "   \n",
    "d1 = spark.read.csv('/user/maxnet/database/sig.db/term_desc_all',sep='\\x01')\n",
    "d1 = d1.withColumnRenamed('_c0','term_id')\\\n",
    "       .withColumnRenamed('_c1','brand')\\\n",
    "       .withColumnRenamed('_c2','cn_name')\\\n",
    "       .withColumnRenamed('_c3','en_name')\\\n",
    "       .withColumnRenamed('_c4','type1')\\\n",
    "       .withColumnRenamed('_c5','type2')\\\n",
    "       .withColumnRenamed('_c6','os')\\\n",
    "       .withColumnRenamed('_c7','dtime')\\\n",
    "       .withColumnRenamed('_c8','price')\\\n",
    "       .withColumnRenamed('_c9','remarks')\\\n",
    "       .withColumnRenamed('_c10','prio')\n",
    "\n",
    "d2 = spark.read.csv('/user/maxnet/database/sig.db/term_oui',sep='\\x01')\n",
    "d2 = d2.withColumnRenamed('_c0','mac_6').withColumnRenamed('_c1','manu')\n",
    "\n",
    "d = spark.read.csv('/user/maxnet/database/sig.db/data_mac_res',sep='\\x01')\n",
    "d = d.withColumnRenamed('_c0','mac').withColumnRenamed('_c1','id')\n",
    "d = d.withColumn('m1_6',substring(d.mac,1,6))\n",
    "\n",
    "tmp_1 = d.join(d2,d.m1_6==d2.mac_6)\n",
    "df = tmp_1.join(d1,tmp_1.id == d1.term_id).select('mac','id','manu','brand','prio').distinct().dropna()\n",
    "df = df.filter(df.prio == 4).select('mac','id','brand').distinct().dropna()\n",
    "\n",
    "df = df.withColumn('m1',substring(df.mac,1,6)).withColumn('m2',substring(df.mac,7,6))\n",
    "df = df.withColumn('p',conv(df.m2, 16, 10))\n",
    "df = df.select('mac','id','m1','m2',col('p').cast('bigint'))\n",
    "d1 = df.filter(df.m1 =='702ED9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    from pyspark.sql.functions import substring,conv,col,min,max,udf,lit\n",
    "   \n",
    "    d1 = spark.read.csv('/user/maxnet/database/sig.db/term_desc_all',sep='\\x01')\n",
    "    d1 = d1.withColumnRenamed('_c0','term_id')\\\n",
    "           .withColumnRenamed('_c1','brand')\\\n",
    "           .withColumnRenamed('_c2','cn_name')\\\n",
    "           .withColumnRenamed('_c3','en_name')\\\n",
    "           .withColumnRenamed('_c4','type1')\\\n",
    "           .withColumnRenamed('_c5','type2')\\\n",
    "           .withColumnRenamed('_c6','os')\\\n",
    "           .withColumnRenamed('_c7','dtime')\\\n",
    "           .withColumnRenamed('_c8','price')\\\n",
    "           .withColumnRenamed('_c9','remarks')\\\n",
    "           .withColumnRenamed('_c10','prio')\n",
    "\n",
    "    d2 = spark.read.csv('/user/maxnet/database/sig.db/term_oui',sep='\\x01')\n",
    "    d2 = d2.withColumnRenamed('_c0','mac_6').withColumnRenamed('_c1','manu')\n",
    "    \n",
    "    d = spark.read.csv('/user/maxnet/database/sig.db/data_mac_res',sep='\\x01')\n",
    "    d = d.withColumnRenamed('_c0','mac').withColumnRenamed('_c1','id')\n",
    "    d = d.withColumn('m1_6',substring(d.mac,1,6))\n",
    "\n",
    "    tmp_1 = d.join(d2,d.m1_6==d2.mac_6)\n",
    "    df = tmp_1.join(d1,tmp_1.id == d1.term_id).select('mac','id','manu','brand','prio').distinct().dropna()\n",
    "    df = df.filter(df.prio == 4).select('mac','id','brand').distinct().dropna()\n",
    "    \n",
    "    df = df.withColumn('m1',substring(df.mac,1,6)).withColumn('m2',substring(df.mac,7,6))\n",
    "    df = df.withColumn('p',conv(df.m2, 16, 10))\n",
    "    df = df.select('mac','id','m1','m2',col('p').cast('bigint'))\n",
    "    d1 = df.filter(df.m1 =='702ED9')\n",
    "    \n",
    "    gp_id_pmin = d1.groupBy('id').agg({'p':'min'})\n",
    "    gp_id_pmax = d1.groupBy('id').agg({'p':'max'}).withColumnRenamed('id','id1')\n",
    "    gp_id_count = d1.groupBy('id').count().withColumnRenamed('id','id2')\n",
    "\n",
    "    stat = gp_id_pmin.join(gp_id_pmax,gp_id_pmin.id ==gp_id_pmax.id1)\n",
    "    stat = stat.join(gp_id_count,stat.id == gp_id_count.id2)\n",
    "    stat = stat.select('id','min(p)','max(p)','count')\n",
    "    stat = stat.withColumn('effect_density',col('count')/(col('max(p)')-col('min(p)')))\n",
    "    stat = stat.fillna(0,'effect_density')\n",
    "    \n",
    "    \n",
    "    bottom = d1.select(min('p')).collect()[0]['min(p)']\n",
    "    top = d1.select(max('p')).collect()[0]['max(p)']\n",
    "    \n",
    "    base = stat.withColumn('x-min(p)',lit(x)-col('min(p)'))\\\n",
    "           .withColumn('x-max(p)',lit(x)-col('max(p)'))\n",
    "\n",
    "    base = base.withColumn('mul',col('x-min(p)')*col('x-max(p)'))\n",
    "\n",
    "    position = udf(lambda x: 0 if x > 0 else 1)\n",
    "\n",
    "    base = base.withColumn('position',position('mul'))\n",
    "    base = base.select('id','min(p)','max(p)','count','effect_density','position').toPandas()   \n",
    "    \n",
    "    if x < bottom:\n",
    "        return base[base['min(p)'] == bottom].iloc[0]['id']\n",
    "    \n",
    "    elif x > top:\n",
    "        return base[base['min(p)'] == top].iloc[0]['id']\n",
    "    \n",
    "    else :\n",
    "        max_effect = base[base.position == '1']['effect_density'].max()\n",
    "        return base[base.effect_density == max_effect].iloc[0]['id']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------+------+-------+\n",
      "|         mac|   id|    m1|    m2|      p|\n",
      "+------------+-----+------+------+-------+\n",
      "|702ED907022B|42980|702ED9|07022B| 459307|\n",
      "|702ED91411FE|42980|702ED9|1411FE|1315326|\n",
      "|702ED93DA3CF|39000|702ED9|3DA3CF|4039631|\n",
      "|702ED94BC16E|42980|702ED9|4BC16E|4964718|\n",
      "+------------+-----+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1.sample(0.001).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3536723, 967250, 3553025, 2836777, 15370372, 458578]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.sample(0.001).select('p').toPandas()['p'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = d1.sample(0.01,seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------+------+--------+\n",
      "|         mac|   id|    m1|    m2|       p|\n",
      "+------------+-----+------+------+--------+\n",
      "|702ED941EC07|48993|702ED9|41EC07| 4320263|\n",
      "|702ED93F3427|48993|702ED9|3F3427| 4142119|\n",
      "|702ED9144F0B|42980|702ED9|144F0B| 1330955|\n",
      "|702ED9915C23|40232|702ED9|915C23| 9526307|\n",
      "|702ED92B67FF|48993|702ED9|2B67FF| 2844671|\n",
      "|702ED9EA580E|10666|702ED9|EA580E|15357966|\n",
      "|702ED9EA0451|10666|702ED9|EA0451|15336529|\n",
      "|702ED93FDBC2|42980|702ED9|3FDBC2| 4185026|\n",
      "|702ED940DAE0|48993|702ED9|40DAE0| 4250336|\n",
      "|702ED923E9D8|42980|702ED9|23E9D8| 2353624|\n",
      "|702ED9412DEE|48993|702ED9|412DEE| 4271598|\n",
      "|702ED908D72B|42980|702ED9|08D72B|  579371|\n",
      "|702ED90DCB9B|42980|702ED9|0DCB9B|  904091|\n",
      "|702ED9BC4C7D|40320|702ED9|BC4C7D|12340349|\n",
      "|702ED911D01A|42980|702ED9|11D01A| 1167386|\n",
      "|702ED906FC7A|42980|702ED9|06FC7A|  457850|\n",
      "|702ED93F361D|48993|702ED9|3F361D| 4142621|\n",
      "|702ED923C0F3|42980|702ED9|23C0F3| 2343155|\n",
      "|702ED976A02E|10666|702ED9|76A02E| 7774254|\n",
      "|702ED907FFC1|42980|702ED9|07FFC1|  524225|\n",
      "+------------+-----+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = []\n",
    "for x in test.select('p').toPandas()['p'].values.tolist():\n",
    "    result.append(predict(x))\n",
    "\n",
    "mac = test.select('mac').toPandas()['mac'].values.tolist()\n",
    "spark.createDataFrame(pd.DataFrame({'mac':mac,'id':result})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.filter(test.mac == '702ED913E0C2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
